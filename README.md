# Econometric Theory: The 100-Day Challenge
You find the code and data used in my playlist on Econometric Theory. Eventually, I will publish the notes as a book in 2024/2025.

# E1: Econometric theory: Join the 100-day challenge
This video introduces the 100-day challenge. Can you master Econometric Theory in 100 days? We start exploring the linear regression model and dive into matrix algebra. Join me!

## [YouTube video 1](https://youtu.be/VIYV92XQTXI)

# E2: System of linear equations in matrix form
This is our first exercise. We will write a system of linear equations in matrix form. We will get used to column vectors and matrices. Dimensions and multiplication are covered.

## [YouTube video 2](https://youtu.be/hgA17G7ocQM)

# E3: Strict exogeneity
This video explores the assumption of strict exogeneity, which is imposed in the context of the linear regression model. 

## [YouTube video 3](https://youtu.be/COKzDxtV-XY)

# E4: The Law of Total Expectations
In exercises, we calculate conditional expected values and explore the Law of Total Expectations.

## [YouTube video 4](https://youtu.be/q7JHXi07vOI)

# E5: No Systematic Error
This video explores the implications of strict exogeneity. We show that error terms of the linear regression model cannot be regarded as systematic. They are uncorrelated with all explanatory variables. This fact follows from strict exogeneity.

## [YouTube video 5](https://youtu.be/ztIBS3Gxqko)

# E6: The data matrix in Python: NumPy
This video introduces NumPy. We will build a data matrix in Python step-by-step. We import NumPy, and introduce arrays. We learn about indexing and assigning arrays. The attributes shape and size of arrays are explained. Finally, we dive into matrix multiplication.

**Chapters**
- 0:00 Intro
- 0:17 NumPy
- 1:37 Arrays
- 2:23 Indexing
- 4:23 Assign Arrays
- 6:14 Shape and Size
- 7:38 Matrix Multiplication
- 8:22 Identity Matrix
- 9:11 Data Matrix

## [YouTube video 6](https://youtu.be/RKaCn1zr7r4)

# E7: An Into to Synthetic Data in Python
Obtaining actual data can be expensive, difficult, or even impossible (e.g., in the case of extreme events). In the absence of a data-generating process, synthetic data might be an option. Synthetic data has many applications in testing and machine learning. This video explores synthetic data in Python.

## [YouTube video 7](https://youtu.be/tV-4JfIJxPc)

# E8: Linear Regression with Simulated Data in Python
Understanding linear regression and Ordinary Least Squares (OLS) begins with clear, visual explanations. In this video, we show how to use simulated data to illustrate the core ideas behind linear regression—without the noise and imperfections of real-world datasets. Why use simulation? Real data often violates key model assumptions due to measurement errors, missing values, or non-linearity. That makes it hard to isolate how a method like OLS is supposed to work. By creating clean, artificial data that strictly follows OLS assumptions, we can see the model’s behaviour under ideal conditions. We walk through the complete process: defining a data-generating process, constructing a linear relationship between variables, fitting a regression model, and visualising the results using a scatter plot and fitted line. 

## [YouTube video 8](https://youtu.be/RBb6tMTBRV4)

# E9:The Theory of OLS Regressions: The Loss Function
Before diving into the technical details of linear algebra, it's essential to grasp the intuition behind fitting a linear model to data. In this video, we break down the basics of Ordinary Least Squares (OLS) regression, a fundamental method in statistics, data science, and econometrics. Using a simple example with one explanatory variable, we guide you through the key question: how do we derive the best-fitting line through a scatterplot of observed data points?

We start by introducing the linear model, where the relationship between the dependent and independent variable is influenced by an error term. The main objective is to estimate the coefficients (intercept and slope) that best describe this relationship. But how do we define "best"? We explore various methods for measuring prediction error, including average error, absolute error, maximum error, and squared error. You’ll learn why the squared error is most commonly used in OLS, as it penalises large deviations and leads to a mathematically convenient solution.

The core of OLS is minimising the sum of squared residuals—the differences between the observed and predicted values. This video walks you through the loss function and sets the stage for solving the minimisation problem analytically. Whether you're a student learning regression for the first time or someone brushing up on core concepts, this video provides a clear, step-by-step explanation of how and why OLS works.

## [YouTube video 9](https://youtu.be/-hPYuxr_7KQ)

# E10: Deriving the Constant Term in Regression Models
We work through a simple example with one explanatory variable. Starting with the sum of squared residuals, we derive the constant term of a regression equation. We notice that the constant term vanishes if variables are standardised by subtracting their mean.

## [YouTube video 10](https://youtu.be/krSsQHWGRZI)

# E11: Deriving the Slope Coefficient in Regression Models
We work through a simple example with one explanatory variable. Starting with the sum of squared residuals, we derive the slope coefficient of the regression line. It is useful to standardise variables by subtracting their mean to remove the need for a constant term.

## [YouTube video 11](https://youtu.be/cgMZ4Gw8nJE)

